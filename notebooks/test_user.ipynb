{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the package\n",
    "import storypy as sp # check if this is used for something else\n",
    "# import esmvaltool\n",
    "# from esmvaltool.diag_scripts.shared import run_diagnostic, get_cfg, group_metadata\n",
    "# from esmvaltool.diag_scripts.shared._base import _get_input_data_files\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To use the esmvaltool configuration file, we need to import the esmvaltool package.\n",
    "This package is not available in the current environment.\n",
    "The following code is an example of how to use the esmvaltool configuration file.\n",
    "'''\n",
    "\n",
    "'''\n",
    "def parse_config(file):\n",
    "    \"\"\"Parse the settings file.\"\"\"\n",
    "    config = get_cfg(file)           \n",
    "    config['input_data'] = _get_input_data_files(config)\n",
    "    return config\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Load the configuration file\n",
    "config= parse_config('/climca/people/ralawode/esmvaltool_output/full_storyline_analysis_complete_20240923_140137/run/multiple_regression_indices/multiple_regresion/settings.yml')'\n",
    "\n",
    "cs.main_esmval(config, user_config)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration dictionary is used to set up the processing parameters for the climate data analysis tool. Each key in the dictionary configures a specific aspect of the data processing pipeline, from specifying file directories to setting analysis periods and regions.\n",
    "\n",
    "## Keys and Descriptions\n",
    "- data_dir\n",
    "    - Type: String\n",
    "    - Description:\n",
    "        - The directory path where the source CMIP6 netCDF files are stored.\n",
    "    - Example: '/climca/data/cmip6-ng'\n",
    "\n",
    "- work_dir\n",
    "    - Type: String\n",
    "    - Description:\n",
    "        - The directory path where intermediate processing outputs and combined netCDF files are written.\n",
    "    - Example: '/climca/people/storylinetool/test_user/work_dir'\n",
    "\n",
    "- plot_dir\n",
    "    - Type: String\n",
    "    - Description:\n",
    "        - The directory path where the generated plots (e.g., time series plots) will be saved.\n",
    "    - Example: '/climca/people/storylinetool/test_user/plot_dir'\n",
    "\n",
    "- var_name\n",
    "    - Type: List of strings\n",
    "    - Description:\n",
    "        - A list of variable names to be processed. These variable names correspond to the variables available in the dataset (e.g., precipitation (pr), sea level pressure (psl)).\n",
    "    - Example: ['pr', 'psl']\n",
    "\n",
    "- exp_name\n",
    "    - Type: String\n",
    "    - Description:\n",
    "        - The experiment name (scenario) to filter scenario files (e.g., climate change scenario identifier).\n",
    "    - Example: 'ssp585'\n",
    "\n",
    "- freq\n",
    "    - Type: String\n",
    "    - Description:\n",
    "        - The frequency of the data, such as monthly ('mon').\n",
    "    - Example: 'mon'\n",
    "\n",
    "- grid\n",
    "    - Type: String\n",
    "    - Description:\n",
    "        - The grid resolution of the dataset (e.g., 'g025' for a 0.25Â° grid).\n",
    "    - Example: 'g025'\n",
    "\n",
    "- region_method\n",
    "    - Type: String\n",
    "    - Description:\n",
    "        - The method to define the region for spatial analysis. For example, using a bounding box method ('box').\n",
    "    - Example: 'box'\n",
    "\n",
    "- period1\n",
    "    - Type: List of strings\n",
    "    - Description:\n",
    "        - The first time period used for the baseline climatology (e.g., historical period). The list contains the start and end years as strings.\n",
    "    - Example: ['1950', '1979']\n",
    "\n",
    "- period2\n",
    "    - Type: List of strings\n",
    "    - Description:\n",
    "        - The second time period used for future projections. This list also contains the start and end years as strings.\n",
    "    - Example: ['2070', '2099']\n",
    "\n",
    "- region_id\n",
    "    - Type: Integer\n",
    "    - Description:\n",
    "        - An identifier for the region being analyzed. This can be used to apply region-specific processing or look up additional metadata.\n",
    "    - Example: 18\n",
    "\n",
    "- season\n",
    "    - Type: List of integers (or tuple)\n",
    "    - Description:\n",
    "        - The months representing the season for which the analysis is performed. This can be provided as a list or a tuple.\n",
    "    - Example: [11, 12, 1, 2, 3]\n",
    "    - Note: You can also supply a tuple, e.g., (12, 1, 2).\n",
    "\n",
    "- region_extents\n",
    "    - Type: List of tuples\n",
    "    - Description:\n",
    "        - A list of tuples, where each tuple defines the geographical bounding box for a region. The tuple values represent (latitude_min, latitude_max, longitude_min, longitude_max).\n",
    "    - Example: [(30, 45, -10, 40), (45, 55, 5, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from storypy.preprocess import DirectProcessor\n",
    "\n",
    "user_config = dict(\n",
    "        data_dir='/climca/people/ralawode/cmip_test',\n",
    "        work_dir='/climca/people/storylinetool/test_user/work_dir',\n",
    "        plot_dir='/climca/people/storylinetool/test_user/plot_dir',\n",
    "        var_name=['psl'],\n",
    "        exp_name='ssp585',\n",
    "        freq='mon',\n",
    "        grid='g025',\n",
    "        region_method='box',\n",
    "        period1 = ['1950', '1979'],\n",
    "        period2 = ['2070', '2099'],\n",
    "        region_id=18,\n",
    "        season=(11, 12, 1, 2, 3, 4),  # Now provided as a tuple of months\n",
    "        region_extents=[(30, 45, -10, 40), (45, 55, 5, 20)],# (-40, 0, -60, -30)],\n",
    "        titles=[\"Region A\", \"Region B\"]\n",
    "    )\n",
    "\n",
    "driver_config = dict(\n",
    "        var_name=['psl'],            # <- actual variable names in NetCDF\n",
    "        short_name=['test_ubi'],           # <- names for regression/CSV outputs\n",
    "        period1=['1960', '1979'],\n",
    "        period2=['2070', '2099'],\n",
    "        season=[12,1,2],\n",
    "        #box={'lat_min': -15, 'lat_max': 15, 'lon_min': -180, 'lon_max': 180}, # ta\n",
    "        box={'lat_min': 50, 'lat_max': 70, 'lon_min': 40, 'lon_max': 70}, \n",
    "        # work_dir='/climca/people/storylinetool/test_user/driver_outputs'\n",
    "        work_dir='/climca/people/storylinetool/test_user/driver_test_outputs'\n",
    "    )\n",
    "\n",
    "'''\n",
    "Run the diagnostics:\n",
    "--------------------\n",
    "- Be sure to have created the work_dir and plot_dir\n",
    "- Running the line below will save the ouput NetCDF file in the work_dir and the plots in the plot_dir.\n",
    "'''\n",
    "processor_target = DirectProcessor(user_config, driver_config)\n",
    "# sp.main_direct(user_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the target and saves the output in the work_dir\n",
    "# If multiple variables are provided, it will compute the target for each variable and save a single file with all variables nested in the work_dir\n",
    "processor_target.process_var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the drivers and saves the output in the work_dir\n",
    "processor_target.process_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users could start from here and import the preprocessed example data that we stored in storypy (za_sh2017, mi2020, mo2023, or gh2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This process the driver outputs and return the results containing the regression coefficients\n",
    "from storypy.compute import compute_drivers_from_netcdf\n",
    "\n",
    "compute_drivers_from_netcdf(driver_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the regression indices (i.e MLR), we can use the following function\n",
    "from storypy.compute import run_regression\n",
    "run_regression(user_config, driver_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regres import spatial_MLR\n",
    "import os\n",
    "\n",
    "def run_regression(preproc, user_config, regressor_csv_path):\n",
    "    \"\"\"\n",
    "    Run spatial multiple linear regression (MLR) using a preprocessed NetCDF dataset and regressors CSV.\n",
    "    \n",
    "    Parameters:\n",
    "        preproc (str): Path to the preprocessed NetCDF file.\n",
    "        user_config (dict): Configuration dictionary containing keys like \"work_dir\".\n",
    "        regressor_csv_path (str): Path to the CSV file containing regressors data.\n",
    "    \n",
    "    This function:\n",
    "      1. Opens the preprocessed NetCDF file.\n",
    "      2. Loads regressors from a CSV file.\n",
    "      3. Finds common models between the dataset and the regressors.\n",
    "      4. Subsets the dataset based on common models.\n",
    "      5. Aligns the regressors DataFrame to the common models.\n",
    "      6. Prepares the regressor names by inserting 'MEM' at the beginning.\n",
    "      7. Instantiates spatial_MLR, sets up regression data, and performs the regression.\n",
    "      8. Saves the regression output to the specified work directory.\n",
    "    \"\"\"\n",
    "\n",
    "    target = user_config[\"target\"]\n",
    "   \n",
    "    ds = xr.open_dataset(preproc)\n",
    "    # Ensure the model coordinate is a string and stripped of any whitespace.\n",
    "    ds_model_names = pd.Index(ds['model'].values.astype(str)).str.strip()\n",
    "    \n",
    " \n",
    "    regressors = pd.read_csv(regressor_csv_path, index_col=0)\n",
    "    regressors.index = regressors.index.str.strip()  # Clean the index if necessary\n",
    "\n",
    "    ds_unique = ds.groupby('model').first()\n",
    "    common_models = list(regressors.index.intersection(ds_unique['model'].values))\n",
    "    print(\"Common models:\", common_models)\n",
    "    \n",
    "    # Subset and reindex using ds_unique.\n",
    "    ds_subset = ds_unique.sel(model=common_models).reindex(model=common_models)\n",
    "    \n",
    "    target_var = user_config.get(\"target\", \"pr\")\n",
    "    target = ds_subset[target_var]\n",
    "\n",
    "    regressors_aligned = regressors.loc[common_models]\n",
    "\n",
    "    regressor_names = regressors_aligned.columns.insert(0, 'MEM')\n",
    "\n",
    "    # Note: spatial_MLR should be defined/imported from your module.\n",
    "    MLR = spatial_MLR()\n",
    "    MLR.regression_data(target, regressors_aligned, regressor_names)\n",
    "\n",
    "    output_path = os.path.join(user_config[\"work_dir\"], 'regression_output')\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    os.chdir(output_path)\n",
    "    MLR.perform_regression(output_path, 'pr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users could decide to start from here and import the preprocessed data that we stored in storypy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_config = dict(\n",
    "        data_dir='/climca/data/cmip6-ng',\n",
    "        work_dir='/climca/people/storylinetool/test_user/work_dir',\n",
    "        plot_dir='/climca/people/storylinetool/test_user/plot_dir',\n",
    "        var_name=['pr'],\n",
    "        target='pr'\n",
    "    )\n",
    "\n",
    "preproc_file = '/climca/people/storylinetool/test_user/work_dir/combined_changes.nc'\n",
    "regressor_csv = '/climca/people/ralawode/esmvaltool_output/zappa_shepherd_CMIP6_20250115_193953/work/storyline_analysis/remote_drivers/remote_drivers/scaled_standardized_drivers.csv'\n",
    "\n",
    "run_regression(preproc_file, user_config, regressor_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def stand(dato):\n",
    "    anom = (dato - np.mean(dato))/np.std(dato)\n",
    "    return anom\n",
    "\n",
    "\n",
    "def compute_drivers_from_netcdf(driver_config):\n",
    "    work_dir = driver_config['work_dir']\n",
    "    var_names = driver_config['var_name']\n",
    "    short_names = driver_config.get('short_name', var_names)\n",
    "\n",
    "    if len(var_names) != len(short_names):\n",
    "        raise ValueError(\"Length of 'var_name' and 'short_name' must match.\")\n",
    "\n",
    "    var_map = dict(zip(short_names, var_names))\n",
    "    driver_files = glob.glob(os.path.join(work_dir, \"*.nc\"))\n",
    "    if not driver_files:\n",
    "        raise FileNotFoundError(f\"No .nc files found in {work_dir}\")\n",
    "\n",
    "    # Initialize structure\n",
    "    model_set = set()\n",
    "    regressor_values = {sn: {} for sn in short_names}\n",
    "\n",
    "    for f in driver_files:\n",
    "        ds = xr.open_dataset(f)\n",
    "\n",
    "        if 'model' not in ds.coords:\n",
    "            print(f\"Skipping file without 'model' coord: {f}\")\n",
    "            continue\n",
    "\n",
    "        for short_name in short_names:\n",
    "            if short_name not in os.path.basename(f):\n",
    "                continue  # Skip this file unless it matches the variable\n",
    "\n",
    "            if short_name not in ds:\n",
    "                print(f\"Variable {short_name} not found in {f}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            for model in ds['model'].values:\n",
    "                model = str(model)\n",
    "                model_set.add(model)\n",
    "                try:\n",
    "                    val = ds[short_name].sel(model=model).mean().item()\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not extract {short_name} for model {model} in {f}: {e}\")\n",
    "                    val = np.nan\n",
    "                regressor_values[short_name][model] = val\n",
    "\n",
    "    # Build dataframe from collected values\n",
    "    models = sorted(model_set)\n",
    "    data = {sn: [regressor_values[sn].get(m, np.nan) for m in models] for sn in short_names}\n",
    "    df_raw = pd.DataFrame(data, index=models)\n",
    "\n",
    "    df_scaled = df_raw.copy()\n",
    "    df_standardized = df_scaled.apply(stand, axis=0)\n",
    "\n",
    "    out_dir = os.path.join(work_dir, \"remote_drivers\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df_raw.to_csv(os.path.join(out_dir, \"drivers.csv\"))\n",
    "    df_scaled.to_csv(os.path.join(out_dir, \"scaled_drivers.csv\"))\n",
    "    df_standardized.to_csv(os.path.join(out_dir, \"scaled_standardized_drivers.csv\"))\n",
    "\n",
    "    print(f\"Saved driver regressors to: {out_dir}\")\n",
    "\"\"\"\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def stand(dato):\n",
    "    \"\"\"\n",
    "    Standardize the data (z-score normalization)\n",
    "    \"\"\"\n",
    "    anom = (dato - np.mean(dato)) / np.std(dato)\n",
    "    return anom\n",
    "\n",
    "def compute_drivers_from_netcdf(driver_config):\n",
    "    \"\"\"\n",
    "    Compute driver values from preprocessed NetCDF files stored in driver_config['work_dir'].\n",
    "\n",
    "    The function handles both variable extraction and global warming scaling.\n",
    "\n",
    "    driver_config:\n",
    "        var_name: list of variable names in NetCDF\n",
    "        short_name: list of output variable names for regression/CSV\n",
    "        work_dir: path where NetCDF files are stored\n",
    "    \"\"\"\n",
    "    work_dir = driver_config['work_dir']\n",
    "    var_names = driver_config['var_name']\n",
    "    short_names = driver_config.get('short_name', var_names)\n",
    "\n",
    "    if len(var_names) != len(short_names):\n",
    "        raise ValueError(\"Length of 'var_name' and 'short_name' must match.\")\n",
    "\n",
    "    # Map short_name to var_name\n",
    "    var_map = dict(zip(short_names, var_names))\n",
    "\n",
    "    # Find all .nc files in the directory\n",
    "    driver_files = glob.glob(os.path.join(work_dir, \"*.nc\"))\n",
    "    if not driver_files:\n",
    "        raise FileNotFoundError(f\"No .nc files found in {work_dir}\")\n",
    "\n",
    "    # Load the global warming index file\n",
    "    gw_file = os.path.join(work_dir, \"remote_driver_gw.nc\")\n",
    "    if not os.path.exists(gw_file):\n",
    "        raise FileNotFoundError(\"Missing required global warming file: remote_driver_gw.nc\")\n",
    "    gw_ds = xr.open_dataset(gw_file)\n",
    "\n",
    "    if 'gw' not in gw_ds:\n",
    "        raise ValueError(\"Global warming dataset must contain variable 'gw'\")\n",
    "\n",
    "    gw_vals = gw_ds['gw']\n",
    "    if 'model' not in gw_vals.coords:\n",
    "        raise ValueError(\"'gw' variable must have 'model' coordinate\")\n",
    "\n",
    "    models = gw_vals['model'].values.tolist()\n",
    "\n",
    "    # Initialize structure for models and regressor values\n",
    "    model_set = set()\n",
    "    regressor_values = {sn: {} for sn in short_names}\n",
    "    regressor_values_scaled = {sn: {} for sn in short_names}\n",
    "\n",
    "    available_model_sets = {sn: set() for sn in short_names}\n",
    "\n",
    "    # Iterate over each NetCDF file\n",
    "    for f in driver_files:\n",
    "        ds = xr.open_dataset(f)\n",
    "\n",
    "        if 'model' not in ds.coords:\n",
    "            print(f\"Skipping file without 'model' coord: {f}\")\n",
    "            continue\n",
    "\n",
    "        # For each short_name, check if it's in the file and process\n",
    "        for short_name in short_names:\n",
    "            if short_name not in os.path.basename(f):\n",
    "                continue  # Skip if the file does not match the variable\n",
    "\n",
    "            if short_name not in ds:\n",
    "                print(f\"Variable {short_name} not found in {f}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract data for each model in the dataset\n",
    "            for model in models:\n",
    "                model = str(model)\n",
    "                model_set.add(model)\n",
    "\n",
    "                try:\n",
    "                    # Extract the mean value of the variable for the model\n",
    "                    val = ds[short_name].sel(model=model).mean().item()\n",
    "                    \n",
    "                    # Get the global warming value for the model\n",
    "                    gw_val = gw_vals.sel(model=model).mean().item()\n",
    "                    \n",
    "                    # Scale the value by the global warming value, if not 0\n",
    "                    scaled_val = val / gw_val if gw_val != 0 else np.nan\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not extract {short_name} for model {model} in {f}: {e}\")\n",
    "                    val = np.nan\n",
    "                    scaled_val = np.nan\n",
    "\n",
    "                # Store the raw and scaled values\n",
    "                regressor_values[short_name][model] = val\n",
    "                regressor_values_scaled[short_name][model] = scaled_val\n",
    "\n",
    "    # Find intersection of model sets for all variables\n",
    "    model_sets = [set(driver.keys()) for driver in regressor_values.values()]\n",
    "    common_models = sorted(set.intersection(*model_sets))\n",
    "    \n",
    "    # Build a dataframe from the collected values\n",
    "    # models = sorted(model_set)\n",
    "    data_raw = {sn: [regressor_values[sn].get(m, np.nan) for m in common_models] for sn in short_names}\n",
    "    data_scaled = {sn: [regressor_values_scaled[sn].get(m, np.nan) for m in common_models] for sn in short_names}\n",
    "\n",
    "    df_raw = pd.DataFrame(data_raw, index=common_models)\n",
    "    df_scaled = pd.DataFrame(data_scaled, index=common_models)\n",
    "\n",
    "    # Standardize the scaled data (z-score normalization)\n",
    "    df_standardized = df_scaled.apply(stand, axis=0)\n",
    "\n",
    "    # Create the output directory and save the dataframes as CSV\n",
    "    out_dir = os.path.join(work_dir, \"remote_drivers\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df_raw.to_csv(os.path.join(out_dir, \"drivers.csv\"))\n",
    "    df_scaled.to_csv(os.path.join(out_dir, \"scaled_drivers.csv\"))\n",
    "    df_standardized.to_csv(os.path.join(out_dir, \"scaled_standardized_drivers.csv\"))\n",
    "\n",
    "    print(f\"Saved driver regressors to: {out_dir}\")\n",
    "    return df_raw, df_scaled, df_standardized\n",
    "\n",
    "driver_config = dict(\n",
    "        var_name=['psl', 'tas', 'psl', 'psl'],            # <- actual variable names in NetCDF\n",
    "        short_name=['ubi', 'utas', 'esi', 'ctp'],           # <- names for regression/CSV outputs\n",
    "        period1=['1960', '1979'],\n",
    "        period2=['2070', '2099'],\n",
    "        # season=[12, 1, 2],\n",
    "        #box={'lat_min': -15, 'lat_max': 15, 'lon_min': -180, 'lon_max': 180}, # ta\n",
    "        box={'lat_min': -90, 'lat_max': 90, 'lon_min': -180, 'lon_max': 180}, # pw\n",
    "        work_dir='/climca/people/storylinetool/test_user/driver_test_outputs'\n",
    "    )\n",
    "\n",
    "\n",
    "df_raw, df_scaled, df_standardized = compute_drivers_from_netcdf(driver_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esmvaltool_user",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
